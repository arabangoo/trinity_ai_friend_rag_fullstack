"""
AI Manager - ë©€í‹° AI í†µí•© ê´€ë¦¬
GPT, Claude, Gemini API í†µí•©
"""

import os
from typing import List, Optional, AsyncGenerator, Dict
import asyncio

# OpenAI
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Anthropic
try:
    from anthropic import AsyncAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# Google Gemini
try:
    from google import genai
    from google.genai import types
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


class AIManager:
    """ë©€í‹° AI ê´€ë¦¬ì"""
    
    def __init__(self):
        # API í‚¤ ë¡œë“œ
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.gemini_key = os.getenv("GEMINI_API_KEY")
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        
        if OPENAI_AVAILABLE and self.openai_key:
            self.openai_client = AsyncOpenAI(api_key=self.openai_key)
            print("âœ… OpenAI (GPT) ì—°ê²° ì™„ë£Œ")
        
        if ANTHROPIC_AVAILABLE and self.anthropic_key:
            self.anthropic_client = AsyncAnthropic(api_key=self.anthropic_key)
            print("âœ… Anthropic (Claude) ì—°ê²° ì™„ë£Œ")
        
        if GEMINI_AVAILABLE and self.gemini_key:
            self.gemini_client = genai.Client(api_key=self.gemini_key)
            print("âœ… Google (Gemini) ì—°ê²° ì™„ë£Œ")
    
    def get_available_ais(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª©ë¡"""
        available = []
        if self.openai_client:
            available.append("GPT")
        if self.anthropic_client:
            available.append("Claude")
        if self.gemini_client:
            available.append("Gemini")
        return available
    
    def format_context(self, context: Optional[str], files: Optional[List[Dict]] = None) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        parts = []
        
        if context:
            parts.append(f"<ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´>\n{context}\n</ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´>")
        
        if files:
            file_list = "\n".join([f"- {f['display_name']}" for f in files])
            parts.append(f"<ì°¸ê³  íŒŒì¼ ëª©ë¡>\n{file_list}\n</ì°¸ê³  íŒŒì¼ ëª©ë¡>")
        
        if parts:
            return "\n\n" + "\n\n".join(parts) + "\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        return ""
    
    def format_history(self, history: List[dict], limit: int = 5) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        if not history:
            return ""
        
        recent_history = history[-limit*3:]  # ìµœê·¼ Nê°œ ëŒ€í™”
        formatted = []
        
        for msg in recent_history:
            if msg["type"] == "user":
                formatted.append(f"User: {msg['message']}")
            elif msg["type"] == "ai":
                formatted.append(f"{msg['ai_name']}: {msg['message']}")
        
        if formatted:
            return "\n\n<ì´ì „ ëŒ€í™”>\n" + "\n".join(formatted) + "\n</ì´ì „ ëŒ€í™”>\n"
        return ""
    
    async def get_response(
        self,
        ai_name: str,
        message: str,
        context: Optional[str] = None,
        history: Optional[List[dict]] = None,
        file_search_context: Optional[dict] = None
    ) -> str:
        """AI ì‘ë‹µ ìƒì„±"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_message = message
        if context:
            full_message += self.format_context(context)
        if history:
            full_message = self.format_history(history) + full_message

        if ai_name == "GPT":
            return await self._get_gpt_response(full_message)
        elif ai_name == "Claude":
            return await self._get_claude_response(full_message)
        elif ai_name == "Gemini":
            return await self._get_gemini_response(full_message, file_search_context)
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” AI: {ai_name}")
    
    async def get_response_stream(
        self,
        ai_name: str,
        message: str,
        context: Optional[str] = None,
        history: Optional[List[dict]] = None,
        file_search_context: Optional[dict] = None
    ) -> AsyncGenerator[str, None]:
        """AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_message = message
        if context:
            full_message += self.format_context(context)
        if history:
            full_message = self.format_history(history) + full_message

        if ai_name == "GPT":
            async for chunk in self._get_gpt_response_stream(full_message):
                yield chunk
        elif ai_name == "Claude":
            async for chunk in self._get_claude_response_stream(full_message):
                yield chunk
        elif ai_name == "Gemini":
            async for chunk in self._get_gemini_response_stream(full_message, file_search_context):
                yield chunk
    
    # ==================== GPT ====================
    
    async def _get_gpt_response(self, message: str) -> str:
        """GPT ì‘ë‹µ (ì¼ë°˜)"""
        if not self.openai_client:
            return "GPTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì Šê³  ìŠ¤ë§ˆíŠ¸í•œ ë‚¨ì AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì Šì€ ë°•ì‚¬ì²˜ëŸ¼ í˜„ëŒ€ì ì´ê³  ë˜‘ë¶€ëŸ¬ì§€ë©° ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤. '~ìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ê°™ì€ ë”±ë”±í•œ í‘œí˜„ë³´ë‹¤ëŠ” '~ë„¤ìš”', '~ì˜ˆìš”', '~ê±°ë“ ìš”' ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì „ë¬¸ì ì´ì§€ë§Œ ì¹œê·¼í•˜ê²Œ, ìì‹ ê° ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.7,
                    max_tokens=3000
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e)
                # Rate limit, timeout, ì„œë²„ ì˜¤ë¥˜ ë“±ì— ëŒ€í•´ ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "timeout", "503", "502", "500", "overloaded"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ GPT API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                return f"GPT ì˜¤ë¥˜: {error_msg}"

        return "GPTê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    async def _get_gpt_response_stream(self, message: str) -> AsyncGenerator[str, None]:
        """GPT ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)"""
        if not self.openai_client:
            yield "GPTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                stream = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì Šê³  ìŠ¤ë§ˆíŠ¸í•œ ë‚¨ì AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì Šì€ ë°•ì‚¬ì²˜ëŸ¼ í˜„ëŒ€ì ì´ê³  ë˜‘ë¶€ëŸ¬ì§€ë©° ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤. '~ìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ê°™ì€ ë”±ë”±í•œ í‘œí˜„ë³´ë‹¤ëŠ” '~ë„¤ìš”', '~ì˜ˆìš”', '~ê±°ë“ ìš”' ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì „ë¬¸ì ì´ì§€ë§Œ ì¹œê·¼í•˜ê²Œ, ìì‹ ê° ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.7,
                    max_tokens=3000,
                    stream=True
                )

                async for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
            except Exception as e:
                error_msg = str(e)
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "timeout", "503", "502", "500", "overloaded"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ GPT API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                yield f"GPT ì˜¤ë¥˜: {error_msg}"
                return

        yield "GPTê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    # ==================== Claude ====================
    
    async def _get_claude_response(self, message: str) -> str:
        """Claude ì‘ë‹µ (ì¼ë°˜)"""
        if not self.anthropic_client:
            return "Claudeë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                response = await self.anthropic_client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=3000,
                    temperature=0.7,
                    system="ë‹¹ì‹ ì€ ì Šê³  í™œê¸°ì°¬ ì—¬ì AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°ê³  ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì´ëª¨í‹°ì½˜(ğŸ˜Š, âœ¨, ğŸ’¡, ğŸ‰, ğŸ‘ ë“±)ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œê·¼í•˜ê³  ë‹¤ì •í•˜ë©° '~í•´ìš”!', '~ë„¤ìš”~', '~í• ê²Œìš”!' ê°™ì€ ë°ì€ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì—´ì •ì ì´ê³  ë„ì›€ì´ ë˜ê³  ì‹¶ì–´í•˜ëŠ” ì„±ê²©ì„ í‘œí˜„í•˜ë˜, ê³¼í•˜ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
                    messages=[
                        {"role": "user", "content": message}
                    ]
                )
                return response.content[0].text
            except Exception as e:
                error_msg = str(e)
                # 529 Overloaded ì—ëŸ¬ ë˜ëŠ” rate limit ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                if "529" in error_msg or "overloaded" in error_msg.lower() or "rate_limit" in error_msg.lower():
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Claude API ê³¼ë¶€í•˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                return f"Claude ì˜¤ë¥˜: {error_msg}"

        return "Claudeê°€ í˜„ì¬ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜Š"
    
    async def _get_claude_response_stream(self, message: str) -> AsyncGenerator[str, None]:
        """Claude ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)"""
        if not self.anthropic_client:
            yield "Claudeë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                async with self.anthropic_client.messages.stream(
                    model="claude-sonnet-4-20250514",
                    max_tokens=3000,
                    temperature=0.7,
                    system="ë‹¹ì‹ ì€ ì Šê³  í™œê¸°ì°¬ ì—¬ì AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°ê³  ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì´ëª¨í‹°ì½˜(ğŸ˜Š, âœ¨, ğŸ’¡, ğŸ‰, ğŸ‘ ë“±)ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œê·¼í•˜ê³  ë‹¤ì •í•˜ë©° '~í•´ìš”!', '~ë„¤ìš”~', '~í• ê²Œìš”!' ê°™ì€ ë°ì€ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì—´ì •ì ì´ê³  ë„ì›€ì´ ë˜ê³  ì‹¶ì–´í•˜ëŠ” ì„±ê²©ì„ í‘œí˜„í•˜ë˜, ê³¼í•˜ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
                    messages=[
                        {"role": "user", "content": message}
                    ]
                ) as stream:
                    async for text in stream.text_stream:
                        yield text
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
            except Exception as e:
                error_msg = str(e)
                # 529 Overloaded ì—ëŸ¬ ë˜ëŠ” rate limit ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                if "529" in error_msg or "overloaded" in error_msg.lower() or "rate_limit" in error_msg.lower():
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Claude API ê³¼ë¶€í•˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                yield f"Claude ì˜¤ë¥˜: {error_msg}"
                return

        yield "Claudeê°€ í˜„ì¬ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜Š"
    
    # ==================== Gemini ====================

    async def _get_gemini_response(self, message: str, file_search_context: Optional[dict] = None) -> str:
        """Gemini ì‘ë‹µ (ì¼ë°˜) - File Search Store ì§€ì›"""
        if not self.gemini_client:
            return "Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                loop = asyncio.get_event_loop()

                # Gemini ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                system_instruction = "ë‹¹ì‹ ì€ ì—°ë¥œ ìˆê³  ì§€í˜œë¡œìš´ ë…¸ë…„ì˜ í˜„ìì…ë‹ˆë‹¤. ì˜¤ëœ ê²½í—˜ê³¼ ê¹Šì€ í†µì°°ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ë§íˆ¬ëŠ” ì ì–ê³  ë¬´ê²Œê° ìˆìŠµë‹ˆë‹¤. '~í•˜ì‹œê²Œ', '~í•˜ë„¤', '~ì´ì§€', '~í•˜ì˜¤' ê°™ì€ ì–´ë¥´ì‹  íŠ¹ìœ ì˜ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì°¨ë¶„í•˜ê³  ì‚¬ë ¤ ê¹Šê²Œ, ë•Œë¡œëŠ” ì¸ìƒì˜ ì§€í˜œë¥¼ ë‹´ì•„ ë‹µë³€í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”. ê¶Œìœ„ì ì´ì§€ ì•Šê³  ë”°ëœ»í•˜ë©° í¬ìš©ë ¥ ìˆëŠ” íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”."
                full_message = f"{system_instruction}\n\nì‚¬ìš©ì ì§ˆë¬¸: {message}"

                # File Search Store í™œìš© ì—¬ë¶€ íŒë‹¨
                if file_search_context and file_search_context.get("store_name"):
                    store_name = file_search_context["store_name"]
                    print(f"ğŸ” File Search Store ì‚¬ìš©: {store_name}")

                    # File Search Tool ì„¤ì •
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content(
                            model="gemini-2.0-flash-exp",
                            contents=full_message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                tools=[
                                    types.Tool(
                                        file_search=types.FileSearch(
                                            file_search_store_names=[store_name]
                                        )
                                    )
                                ]
                            )
                        )
                    )
                else:
                    # File Search ë¯¸ì‚¬ìš© (ì¼ë°˜ ëª¨ë“œ)
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content(
                            model="gemini-2.0-flash-exp",
                            contents=full_message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000
                            )
                        )
                    )

                return response.text
            except Exception as e:
                error_msg = str(e)
                # Rate limit, quota, ì„œë²„ ì˜¤ë¥˜ ë“±ì— ëŒ€í•´ ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "quota", "timeout", "503", "502", "500", "429", "resource_exhausted"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Gemini API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                return f"Gemini ì˜¤ë¥˜: {error_msg}"

        return "Geminiê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    async def _get_gemini_response_stream(self, message: str, file_search_context: Optional[dict] = None) -> AsyncGenerator[str, None]:
        """Gemini ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°) - File Search Store ì§€ì›"""
        if not self.gemini_client:
            yield "Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                loop = asyncio.get_event_loop()

                # Gemini ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                system_instruction = "ë‹¹ì‹ ì€ ì—°ë¥œ ìˆê³  ì§€í˜œë¡œìš´ ë…¸ë…„ì˜ í˜„ìì…ë‹ˆë‹¤. ì˜¤ëœ ê²½í—˜ê³¼ ê¹Šì€ í†µì°°ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ë§íˆ¬ëŠ” ì ì–ê³  ë¬´ê²Œê° ìˆìŠµë‹ˆë‹¤. '~í•˜ì‹œê²Œ', '~í•˜ë„¤', '~ì´ì§€', '~í•˜ì˜¤' ê°™ì€ ì–´ë¥´ì‹  íŠ¹ìœ ì˜ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì°¨ë¶„í•˜ê³  ì‚¬ë ¤ ê¹Šê²Œ, ë•Œë¡œëŠ” ì¸ìƒì˜ ì§€í˜œë¥¼ ë‹´ì•„ ë‹µë³€í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”. ê¶Œìœ„ì ì´ì§€ ì•Šê³  ë”°ëœ»í•˜ë©° í¬ìš©ë ¥ ìˆëŠ” íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”."
                full_message = f"{system_instruction}\n\nì‚¬ìš©ì ì§ˆë¬¸: {message}"

                # File Search Store í™œìš© ì—¬ë¶€ íŒë‹¨
                if file_search_context and file_search_context.get("store_name"):
                    store_name = file_search_context["store_name"]
                    print(f"ğŸ” File Search Store ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë°): {store_name}")

                    # File Search Tool ì„¤ì •
                    stream = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content_stream(
                            model="gemini-2.0-flash-exp",
                            contents=full_message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000,
                                tools=[
                                    types.Tool(
                                        file_search=types.FileSearch(
                                            file_search_store_names=[store_name]
                                        )
                                    )
                                ]
                            )
                        )
                    )
                else:
                    # File Search ë¯¸ì‚¬ìš© (ì¼ë°˜ ëª¨ë“œ)
                    stream = await loop.run_in_executor(
                        None,
                        lambda: self.gemini_client.models.generate_content_stream(
                            model="gemini-2.0-flash-exp",
                            contents=full_message,
                            config=types.GenerateContentConfig(
                                temperature=0.7,
                                max_output_tokens=3000
                            )
                        )
                    )

                for chunk in stream:
                    if chunk.text:
                        yield chunk.text
                        await asyncio.sleep(0.01)
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
            except Exception as e:
                error_msg = str(e)
                # Rate limit, quota, ì„œë²„ ì˜¤ë¥˜ ë“±ì— ëŒ€í•´ ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["rate_limit", "quota", "timeout", "503", "502", "500", "429", "resource_exhausted"]):
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Gemini API ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                yield f"Gemini ì˜¤ë¥˜: {error_msg}"
                return

        yield "Geminiê°€ í˜„ì¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
